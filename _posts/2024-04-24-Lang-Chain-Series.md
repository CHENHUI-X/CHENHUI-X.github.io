---
title: Lang Chain Series
date: 2024-04-24 19:0000 +0800
categories: [Deep Learning, LLM, LangChain]
tags: [deep learning, llm, langchain] # TAG names should always be lowercase
math: true
---

## 0. å‰è¨€

æœ¬ç¯‡ Blog ä¸æ˜¯æ•™ç¨‹, å®˜æ–¹æ•™ç¨‹[1](https://python.langchain.com/docs/modules/)ã€[2](https://python.langchain.com/docs/get_started/introduction) æœ‰æ—¶å€™æ¯”è¾ƒæŠ½è±¡, è¿™é‡Œåªæ˜¯åœ¨å­¦ä¹  LangChain è¿‡ç¨‹ä¸­åšä¸ªè®°å½•, å¹¶åŠ å…¥è‡ªå·±çš„ç†è§£å’Œæ³¨é‡Š. å½“ç„¶è¿™é‡Œä¹Ÿä¸è¿›è¡Œè¿‡å¤šçš„ä»‹ç», ç›´æ¥è¿›å…¥å¯¹ç»„ä»¶çš„å­¦ä¹ .

## 1. Prompts

Prompts é€šå¸¸ç”¨æ¥"è°ƒæ•™"LLM, æ¯”å¦‚ç”¨æ¥æŒ‡å®š LLM çš„è¾“å‡ºæ ¼å¼, ä¹Ÿå¯ä»¥ç”¨æ¥ç»™ LMM ä¸€äº›ä¾‹å­è®©ä»–å‚è€ƒç­‰ç­‰.LangChain ç›®å‰æä¾›äº† 4 ç§ Prompts template , æ–¹ä¾¿ç”¨æˆ·æ„é€  Prompt.

### 1.1 PromptTemplate

PromptTemplate æ˜¯æœ€ç®€å•, æœ€åŸºæœ¬çš„ä¸€ç§ Template. API Reference:[PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

å®˜æ–¹ç»™äº† 2 ç§æ–¹æ³•æ¥ç”¨ PromptTemplate.

- æ–¹æ³•ä¸€(æ¨è)

```python
from langchain_core.prompts import PromptTemplate

# Instantiation using from_template (recommended)
prompt = PromptTemplate.from_template("Say {foo}")
prompt.format(foo="bar")
```

ç»“æœ : `Say bar`

- æ–¹æ³•äºŒ

```python
from langchain_core.prompts import PromptTemplate

# Instantiation using initializer
prompt = PromptTemplate(input_variables=["foo"],  template="Say {foo}")
prompt.format(foo="bar")

```

ç»“æœ : `Say bar`

### 1.2 ChatPromptTemplate

ChatPromptTemplate é€šå¸¸æœ‰ 3 ç§è§„åˆ™: "system", "ai" and "human". API Reference:[ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)

```python

from langchain_core.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        ("system",  "You are a helpful AI bot. Your name is {name}."),
        ("human",  "Hello,  how are you doing?"),
        ("ai",  "I'm doing well,  thanks!"),
        ("human",  "{user_input}"),
    ]
)

messages = chat_template.format_messages(name="Bob",  user_input="What is your name?")
```

è¾“å‡º:

```python

[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'),
 HumanMessage(content='Hello,  how are you doing?'),
 AIMessage(content="I'm doing well,  thanks!"),
 HumanMessage(content='What is your name?')]
```

å¯ä»¥çœ‹åˆ°, å®é™…æ˜¯äº§ç”Ÿäº† `SystemMessage`, `HumanMessage` and `AIMessage` å…± 3 ç§ Message. ä¸ä¹‹å¯¹åº”çš„, åœ¨æ„é€ è¿™äº› Message æ—¶, å¯ä»¥ä½¿ç”¨ç›¸åº”çš„ Template : `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`

ä¾‹å­:

```python
from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ª{llm_type}"),  # ä½¿ç”¨ templateæ„é€  ç³»ç»Ÿçš„ message
        ('ai', "å¾ˆé«˜å…´å¸®åŠ©æ‚¨."),  # ç›´æ¥ä½¿ç”¨ role æ„é€  AIçš„ message
        HumanMessagePromptTemplate.from_template("{text}"),
    ]
)
messages = chat_template.format_messages(llm_type="AIåŠ©æ‰‹", text = "1 + 1 = ?")
print(messages)

```

è¾“å‡º:

```python
[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹'),
AIMessage(content='å¾ˆé«˜å…´å¸®åŠ©æ‚¨.'),
 HumanMessage(content='1 + 1 = ?')]
```

### 1.3 Example selectors

å‡è®¾æœ‰ä¸€äº›ä¾‹å­, æˆ‘ä»¬å¸Œæœ› LLM èƒ½å¤Ÿæ ¹æ®è¾“å…¥æŒ‘ä¸€äº›åˆé€‚çš„ä¾‹å­å‡ºæ¥, æ–¹ä¾¿æˆ‘ä»¬åç»­çš„æ“ä½œ, æ¯”å¦‚å°†ä»–ä»¬æ”¾åˆ°ä¸€ä¸ª prompt ä¸­. [API Reference](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)

#### Select by length

ç®€å•æ¥è¯´, è¿™ä¸ª selector æ˜¯æ ¹æ®ç”¨æˆ·è¾“å…¥çš„è¯­å¥é•¿çŸ­é€‰æ‹©åˆé€‚çš„ example, ä½ è¾“å…¥çš„è¶ŠçŸ­,ä»–ç»™çš„ä¾‹å­è¶Šå¤š, ä½ è¾“å‡ºçš„è¶Šé•¿,ä»–ç»™çš„ä¾‹å­è¶Šå°‘.

ä¾‹å­:

```python
from langchain_core.example_selectors import LengthBasedExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)
example_selector = LengthBasedExampleSelector(
    # The examples it has available to choose from.
    examples=examples,

    # The PromptTemplate being used to format the examples.
    # è¿™ä¸ªå‚æ•°æœ‰äº› selector ä¸éœ€è¦, æœ‰äº›æ˜¯å¿…é¡»çš„, è¯·å‚è€ƒå‡½æ•°å…·ä½“API
    example_prompt=example_prompt,

    # The maximum length that the formatted examples should be.
    # Length is measured by the get_text_length function below.
    max_length=25,
    # The function used to get the length of a string, which is used
    # to determine which examples to include. It is commented out because
    # it is provided as a default value if none is specified.
    # get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))
)
dynamic_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)

```

è¾“å…¥:

```python
# An example with small input, so it selects all examples.
print(dynamic_prompt.format(adjective="big"))
```

è¾“å‡º:

```plaintext
Give the antonym of every input

Input: happy
Output: sad

Input: tall
Output: short

Input: energetic
Output: lethargic

Input: sunny
Output: gloomy

Input: windy
Output: calm

Input: big
Output:
```

è¾“å…¥:

```python
# An example with long input, so it selects only one example.
long_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
print(dynamic_prompt.format(adjective=long_string))
```

è¾“å‡º:

```plaintext
# å¯ä»¥çœ‹åˆ°åªç»™äº†ä¸€ä¸ªexample
Give the antonym of every input

Input: happy
Output: sad

Input: big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else
Output:
```

#### Select by maximal marginal relevance (MMR)

è¿™ä¸ª selector çš„æ€æƒ³æ˜¯, é€‰æ‹©ä¸å½“å‰è¾“å…¥ p ç›¸ä¼¼çš„(cosine similarity) example $q_j$ , ä½†æ˜¯è¿™ä¸ª $q_j$ è¿˜è¦å°½é‡ä¸ example pool ä¸­ $q_i$ ä¸è¦å¤ªç›¸ä¼¼, è¿™æ˜¯ä¸ºäº†å¤šæ ·æ€§. [åŸå§‹ paper](https://arxiv.org/pdf/2211.13892.pdf) ä¸­çš„ next example é€‰æ‹©å…¬å¼ä¸º:

![image.png](https://s2.loli.net/2024/04/25/2bHXmLKFRyAkxl5.png)

å¯ä»¥çœ‹åˆ°, å¦‚æœä¸‹ä¸€ä¸ª $q_j$ å’Œ å½“å‰çš„ $p$ å¾ˆç›¸ä¼¼, ä½†æ˜¯å’Œå…¶ä»–çš„ $q_i$ ä¹Ÿéå¸¸ç›¸ä¼¼, é‚£ä¹ˆè¿™ä¸ªåˆ†æ•°ä¹Ÿä¸ä¼šå¤ªé«˜.

ä¾‹å­

```python
from langchain_community.vectorstores import FAISS
from langchain_core.example_selectors import (
    MaxMarginalRelevanceExampleSelector,
    SemanticSimilarityExampleSelector,
)
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]
example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    FAISS,
    # The number of examples to produce.
    k=2,
)
mmr_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
```
è¾“å…¥:
```python
# Input is a feeling, so should select the happy/sad example as the first one
print(mmr_prompt.format(adjective="worried"))
```

è¾“å‡º:

```plaintext
Give the antonym of every input

Input: happy
Output: sad

Input: windy
Output: calm

Input: worried
Output:
```

#### Select by similarity

è¿™ä¸ªå°±å¾ˆå•çº¯äº†, ç›´æ¥ä½¿ç”¨ cos similarity æ¥é€‰æ‹©æœ€ä½³çš„ example, å…¸å‹ selector æ˜¯ SemanticSimilarityExampleSelector.

ä¾‹å­

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]
example_selector = SemanticSimilarityExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # The number of examples to produce.
    k=1,
)
similar_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)
```
è¾“å…¥:
```python
# Input is a feeling, so should select the happy/sad example
print(similar_prompt.format(adjective="worried"))
```
è¾“å‡º

```plaintext
Give the antonym of every input

Input: happy
Output: sad

Input: worried
Output:
```

#### Select by n-gram overlap

è¿™ä¸ªæ˜¯è®¡ç®—è¾“å…¥çš„ query å’Œ example çš„ similarity(0-1 ä¹‹é—´), ç„¶åæ ¹æ®ç»™å®šçš„é˜ˆå€¼, ç»™å‡ºæ»¡è¶³æ¡ä»¶çš„ example.

é˜ˆå€¼ä¸º 0.0 è¡¨ç¤ºåªæ’é™¤ä¸ç›¸å…³çš„ example. é˜ˆå€¼ä¸º -1.0, è¡¨ç¤ºæ‰€æœ‰çš„ example éƒ½ä¼šè¿”å›. å¤§äº 1.0 è¡¨ç¤ºä¸è¿”å› example, é»˜è®¤ä¸º -1.0

ä¾‹å­

```python
from langchain_community.example_selector.ngram_overlap import (
    NGramOverlapExampleSelector,
)
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a fictional translation task.
examples = [
    {"input": "See Spot run.", "output": "Ver correr a Spot."},
    {"input": "My dog barks.", "output": "Mi perro ladra."},
    {"input": "Spot can run.", "output": "Spot puede correr."},
]
example_selector = NGramOverlapExampleSelector(
    # The examples it has available to choose from.
    examples=examples,
    # The PromptTemplate being used to format the examples.
    example_prompt=example_prompt,
    # The threshold, at which selector stops.
    # It is set to -1.0 by default.
    threshold=-1.0,
    # For negative threshold:
    # Selector sorts examples by ngram overlap score, and excludes none.
    # For threshold greater than 1.0:
    # Selector excludes all examples, and returns an empty list.
    # For threshold equal to 0.0:
    # Selector sorts examples by ngram overlap score,
    # and excludes those with no ngram overlap with input.
)
dynamic_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the Spanish translation of every input",
    suffix="Input: {sentence}\nOutput:",
    input_variables=["sentence"],
)

```
è¾“å…¥:
```python
# An example input with large ngram overlap with "Spot can run."
# and no overlap with "My dog barks."
print(dynamic_prompt.format(sentence="Spot can run fast."))
```
è¾“å‡º:

```plaintext
# å¯ä»¥çœ‹åˆ°, å³ä½¿ "My dog barks." ä¸è¾“å…¥ä¸ç›¸å…³, ä½†è¿˜æ˜¯è¾“å‡ºäº†
Give the Spanish translation of every input

Input: Spot can run.
Output: Spot puede correr.

Input: See Spot run.
Output: Ver correr a Spot.

Input: My dog barks.
Output: Mi perro ladra.

Input: Spot can run fast.
Output:
```

è¾“å…¥:

```python
# ä¸è¾“å‡º ä¸ç›¸å…³çš„
example_selector.threshold = 0.0
print(dynamic_prompt.format(sentence="Spot can run fast."))

```

è¾“å‡º:

```plaintext
Give the Spanish translation of every input

Input: Spot can run.
Output: Spot puede correr.

Input: See Spot run.
Output: Ver correr a Spot.

Input: Spot plays fetch.
Output: Spot juega a buscar.

Input: Spot can run fast.
Output:
```

### 1.3 Few-shot prompt templates

ä¸Šè¾¹ä»‹ç»äº†ä¸€äº› example selector , ç°åœ¨ä»‹ç»
FewShotPromptTemplate + example selector .
äºŒè€…å®ç°çš„åŠŸèƒ½å°±æ˜¯, é¦–å…ˆæˆ‘ä»¬æœ‰ä¸€å † example (é€šå¸¸æ˜¯æˆå¯¹å„¿çš„è¾“å…¥å’Œè¾“å‡º) , ç„¶åå¯ä»¥è‡ªé€‚åº”çš„, æ ¹æ®ä¸åŒçš„è¾“å…¥, èƒ½å¤Ÿè‡ªåŠ¨çš„ select åˆé€‚çš„ example ä¸ è¾“å…¥åˆå¹¶, ä¸€åŒå˜ä¸º LLM çš„ prompt.

ä¹Ÿå°±æ˜¯è¯´, ä¸åŒçš„è¾“å…¥, ä¼šäº§ç”Ÿä¸åŒçš„ prompt , æˆ‘ç†è§£æ˜¯å’Œ Retrieval-augmented generation (RAG) ç±»ä¼¼çš„æ•ˆæœ.

define example:

```python
from langchain_core.prompts.few_shot import FewShotPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

examples = [
    {
        "question": "Who lived longer, Muhammad Ali or Alan Turing?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
""",
    },
    {
        "question": "When was the founder of craigslist born?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952
""",
    },
    {
        "question": "Who was the maternal grandfather of George Washington?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
""",
    },
    {
        "question": "Are both the directors of Jaws and Casino Royale from the same country?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No
""",
    },
]

# format example pool
example_prompt = PromptTemplate(
    input_variables=["question", "answer"], template="Question: {question}\n{answer}"
)

```

define selector:

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # This is the list of examples available to select from.
    examples,
    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # This is the number of examples to produce.
    k=1,
)

```

FewShotPromptTemplate + example + selector :

```python
prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

print(prompt.format(input="Who was the father of Mary Ball Washington?"))
```

è¾“å‡º:

```plaintext
# å¯ä»¥çœ‹åˆ° prompt æœ€ååªå¼•å…¥äº†ä¸€ä¸ª example

Question: Who was the maternal grandfather of George Washington?

Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball


Question: Who was the father of Mary Ball Washington?
```

### 1.5 Partial prompt templates

è¿™ä¸ªåŠŸèƒ½å°±æ˜¯ç±»ä¼¼å‡½æ•°çš„å‚æ•°å…·æœ‰é»˜è®¤å€¼.

ä¾‹å­:

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("{foo} {bar}")
partial_prompt = prompt.partial(foo="666")
print(partial_prompt.format(bar="baz")) # è¾“å‡º 666 baz
```

è¿™ä¸ªä¹Ÿå¯ä»¥ä½¿ç”¨å‡½æ•°:

```python
from datetime import datetime

def _get_datetime():
    now = datetime.now()
    return now.strftime("%m/%d/%Y, %H:%M:%S")

prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective", "date"],
)
# date é»˜è®¤ è°ƒç”¨å½“å‰æ—¶é—´
partial_prompt = prompt.partial(date=_get_datetime)
# è¾“å‡º : Tell me a funny joke about the day 04/25/2024, 23:22:18
print(partial_prompt.format(adjective="funny"))

```

### 1.6 PipelinePrompt

PipelinePrompt èƒ½å¤ŸæŠŠå¤šä¸ª prompt æ•´åˆ°ä¸€èµ·.

```python
from langchain_core.prompts.pipeline import PipelinePromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

# æœ€ç»ˆçš„ prompt
full_template = """{introduction}

{example}

{start}"""
full_prompt = PromptTemplate.from_template(full_template)

# ç”¨äº Introduction
introduction_template = """You are impersonating {person}."""
introduction_prompt = PromptTemplate.from_template(introduction_template)

# example
example_template = """Here's an example of an interaction:

Q: {example_q}
A: {example_a}"""
example_prompt = PromptTemplate.from_template(example_template)

# ç”¨æˆ·çš„è¾“å…¥
start_template = """Now, do this for real!

Q: {input}
A:"""
start_prompt = PromptTemplate.from_template(start_template)

# å°†ä¸Šè¾¹çš„ prompt åˆå¹¶åˆ°ä¸€èµ·
input_prompts = [
    ("introduction", introduction_prompt),
    ("example", example_prompt),
    ("start", start_prompt),
]
# æŒ‡å®šè°æ˜¯è°
pipeline_prompt = PipelinePromptTemplate(
    final_prompt=full_prompt, pipeline_prompts=input_prompts
)
print(
    pipeline_prompt.format(
        person="Elon Musk",
        example_q="What's your favorite car?",
        example_a="Tesla",
        input="What's your favorite social media site?",
    )
)
```

è¾“å‡º :

```plaintext

You are impersonating Elon Musk.

Here's an example of an interaction:

Q: What's your favorite car?
A: Tesla

Now, do this for real!

Q: What's your favorite social media site?
A:

```

## 2. Retrieval

Retrieval Augmented Generation (RAG) å¯èƒ½æ˜¯ç›®å‰ LLM å‘æŒ¥æ¯”è¾ƒå¤§ä½œç”¨çš„ä¸€ä¸ªåº”ç”¨. å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤–æŒ‚çš„çŸ¥è¯†åº“èµ‹äºˆåœ¨ä¸åŒçš„å‚ç›´é¢†åŸŸåº”ç”¨èƒ½åŠ›.

å…¶æ ¸å¿ƒæµç¨‹å¦‚ä¸‹:

[1] é¦–å…ˆæˆ‘ä»¬è¦æœ‰ç›¸åº”çš„èµ„æºåº“, source

[2] ç„¶åé’ˆå¯¹ä¸åŒçš„èµ„æº, æˆ‘ä»¬ä½¿ç”¨ç›¸åº”çš„ dataloader å°†èµ„æºæ•°æ®è¯»å–

[3] ç”±äºèµ„æºæ–‡æ¡£æ¯”è¾ƒé•¿, é€šå¸¸æˆ‘ä»¬è¦è¿›è¡Œåˆ†å—, ç§°ä¸º chunk

[4] å°†æ–‡æ¡£ chunk å, ä¼šå¯¹æ¯ä¸ª chunk è¿›è¡Œ embedding

[5] embedding ä¹‹å, è¦è¿›è¡Œ store, è¿™ä¸ªç»„ä»¶ä¸€èˆ¬ç§°ä¸º vector store

[6] å½“æˆ‘ä»¬ç»™å®šè¾“å…¥çš„æ—¶å€™, LLM èƒ½å¤Ÿæ ¹æ®è¯­ä¹‰ä» store ä¸­æŠ½å–æœ‰ç”¨çš„èµ„æº,è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ retrieve

![image.png](https://s2.loli.net/2024/04/26/IiHL1MVWNc8QJtG.png)

### 2.1 Dataloader

[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/document_loaders/)é›†æˆäº†å¾ˆå¤šç¬¬ä¸‰æ–¹ dataloader,
ç”šè‡³å¯ä»¥ç›´æ¥ä» arxivã€GitHub ç­‰ç›´æ¥è·å–æ•°æ®. ä½†æ˜¯å¸¸ç”¨çš„å¯èƒ½å°±æ˜¯é’ˆå¯¹ æ–‡æœ¬ å’Œ csv çš„, è€Œä¸”ä½¿ç”¨æ–¹æ³•ç±»ä¼¼, æ‰€ä»¥è¿™é‡Œåªå­¦ä¹  æ–‡æœ¬ç±»å‹ çš„.

#### Document Loader

LangChain ç»™çš„ä¾‹å­æ˜¯ç»§æ‰¿ BaseLoader, ç„¶åå°†è¯»åˆ°çš„æ–‡æœ¬åˆå§‹åŒ–ä¸º Document å¯¹è±¡.
å†…éƒ¨æœ‰ 4 ä¸ªåŸºæœ¬æ–¹æ³•: ç›´æ¥è¯»å–æ‰€æœ‰, å¼‚æ­¥è¯»å–æ‰€æœ‰, lazy è¯»å–, å¼‚æ­¥ lazy è¯»å–.

![image.png](https://s2.loli.net/2024/04/26/PC8fvdnsaHA9KB6.png){: width="400" height="300" }

ä¾‹å­:

<details markdown="1">
<summary> è¯¦ç»†ä»£ç  </summary>

```python
from typing import AsyncIterator, Iterator
from langchain_core.document_loaders import BaseLoader
from langchain_core.documents import Document

class CustomDocumentLoader(BaseLoader):
    """An example document loader that reads a file line by line."""

    def __init__(self, file_path: str) -> None:
        """Initialize the loader with a file path.

        Args:
            file_path: The path to the file to load.
        """
        self.file_path = file_path

    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments
        """A lazy loader that reads a file line by line.

        When you're implementing lazy load methods, you should use a generator
        to yield documents one by one.
        """
        with open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

    # alazy_load is OPTIONAL.
    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!
    async def alazy_load(
        self,
    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments
        """An async lazy loader that reads a file line by line."""
        # Requires aiofiles
        # Install with `pip install aiofiles`
        # https://github.com/Tinche/aiofiles
        import aiofiles

        async with aiofiles.open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            async for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

with open("./meow.txt", "w", encoding="utf-8") as f:
    quality_content = "meow meowğŸ± \n meow meowğŸ± \n meowğŸ˜»ğŸ˜»"
    f.write(quality_content)

loader = CustomDocumentLoader("./meow.txt")

## Test out the lazy load interface
for doc in loader.lazy_load():
    print()
    print(type(doc))
    print(doc)

## Test out the async implementation
async for doc in loader.alazy_load():
    print()
    print(type(doc))
    print(doc)

"""
è¾“å‡ºç»“æœä¸€æ ·:

<class 'langchain_core.documents.base.Document'>
page_content='meow meowğŸ± \n' metadata={'line_number': 0, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meow meowğŸ± \n' metadata={'line_number': 1, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meowğŸ˜»ğŸ˜»' metadata={'line_number': 2, 'source': './meow.txt'}
"""
```

</details>

> load() can be helpful in an interactive environment such as a jupyter notebook.
> Avoid using it for production code since eager loading assumes that all the content can fit into memory, which is not always the case, especially for enterprise data.
> {: .prompt-info }

### 2.2 Text Splitters

Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window.

[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/document_transformers/)ç»™äº†å¤šç§ Splitter, æœ€å¸¸ç”¨çš„ä¸ºä»¥ä¸‹ 3 ç§.

#### Split by character

è¿™ä¸ªæ˜¯æœ€ç®€å•çš„ Splitter, å•çº¯å°±æ˜¯ä½¿ç”¨æŒ‡å®šçš„ character å»åˆ‡åˆ† text .

ä¾‹å­:

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator=" ", # æŒ‡å®š separator
    chunk_size=5,
    chunk_overlap=2,
    length_function=len,
    is_separator_regex=False,
)

text_splitter.split_text("æˆ‘æ˜¯ç»ƒä¹ æ—¶é•¿ è¾¾ä¸¤å¹´åŠçš„å¤å¤")

```

ä¸Šè¿°ä»£ç ä¸­, chunk_size æœ¬æ„æ˜¯æŒ‡è¿›è¡Œ split ä¹‹å, åç»­è¿›è¡Œ store çš„æ—¶å€™, æœ€å¤§ä»¥å¤šå¤§çš„ size ä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œå­˜å‚¨. ä½†æ˜¯å¯ä»¥çœ‹åˆ°è¿™ä¸ªå‚æ•°å¯¹äº CharacterTextSplitter ä¸ç”Ÿæ•ˆ, å®é™…ä¸Š[æºç ](https://api.python.langchain.com/en/latest/_modules/langchain_text_splitters/character.html#CharacterTextSplitter)ä¸­, å°±æ˜¯ç®€å•çš„ç”¨ re.split() å¯¹æ–‡æ¡£æŒ‰ç…§ separator è¿›è¡Œåˆ‡å‰², ä¸ç®¡å­å¥æœ‰å¤šé•¿, ç›´æ¥è¿”å›.

è¾“å‡º:

```plaintext
['æˆ‘æ˜¯ç»ƒä¹ æ—¶é•¿', 'è¾¾ä¸¤å¹´åŠçš„å¤å¤']
```

> å› ä¸º LLM é€šå¸¸å¯¹è¾“å…¥æœ‰é•¿åº¦é™åˆ¶, å› æ­¤ CharacterTextSplitter ä¸å¤ªé€‚åˆ, å¯èƒ½ä¼šè¶…å‡ºè¾“å…¥å°ºå¯¸èŒƒå›´, è€Œä¸‹è¾¹çš„ RecursiveCharacterTextSplitter å¯ä»¥é€’å½’åˆ‡å‰²å­å¥, ç›´åˆ°æ¯ä¸ªå­å¥éƒ½å°äº chunk size.
> {: .prompt-info }

#### Recursive Splitter By Character

è¿™ä¸ªçœ‹äº†[æºç ](https://api.python.langchain.com/en/latest/_modules/langchain_text_splitters/character.html#RecursiveCharacterTextSplitter),å®ƒé»˜è®¤çš„ `separator = ["\n\n", "\n", " ", ""]`, æˆ‘çš„ç†è§£æ˜¯è¯´, é¦–å…ˆä¼šæ ¹æ®`\n\n`è¿›è¡Œåˆ‡å‰². ä¸€èˆ¬æ¥è¯´, 2 ä¸ªæ¢è¡Œåˆ†å‰²å¼€çš„é€šå¸¸æ˜¯ 2 ç¯‡æ–‡ç« . æ‰€ä»¥ä¼šå…ˆæŒ‰ç…§è¿™ä¸ªå°ºåº¦è¿›è¡Œåˆ‡å‰². å¦‚æœåˆ‡å‰²ä¹‹å, æŸç¯‡æ–‡ç« è¿˜æ˜¯å¤ªé•¿(å¤§äº chunk size), é‚£ä¹ˆä¼šç»§ç»­ä½¿ç”¨ `\n` è¿›è¡Œåˆ’åˆ†åˆ‡å‰², åŒç†ç›´åˆ°å…¶é•¿åº¦å°äº chunk size.

ä¾‹å­:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=5,
    chunk_overlap=0,
    length_function=len,
    is_separator_regex=False,
    keep_separator = False
)
text_splitter.split_text(
    "æˆ‘æ˜¯\n\nç»ƒä¹ æ—¶é•¿è¾¾ä¸¤å¹´\nåŠçš„å¤å¤"
    )
```

è¾“å‡º:

```plaintext
['æˆ‘æ˜¯', 'ç»ƒä¹ æ—¶é•¿è¾¾', 'ä¸¤å¹´', 'åŠçš„å¤å¤']
# å¯ä»¥çœ‹åˆ°, é¦–å…ˆç”¨`\n\n`è¿›è¡Œåˆ†å‰², å› ä¸º"æˆ‘æ˜¯"çš„é•¿åº¦å°äº5, æ‰€ä»¥ç›´æ¥å­˜èµ·æ¥,
# ä½†æ˜¯åè¾¹éƒ¨åˆ†å¤ªé•¿, åˆåŸºäº`\n`è¿›è¡Œåˆ‡å‰², "åŠçš„å¤å¤"æ˜¯æ»¡è¶³è¦æ±‚çš„,æ‰€ä»¥ä¸€èµ·å­˜äº†èµ·æ¥.
# ä½†æ˜¯"ç»ƒä¹ æ—¶é•¿è¾¾ä¸¤å¹´"çš„é•¿åº¦è¿˜æ˜¯å¤§äº5, äºæ˜¯è¿›è¡Œäº†ç»§ç»­çš„åˆ‡å‰². å˜ä¸º"ç»ƒä¹ æ—¶é•¿è¾¾" å’Œ "ä¸¤å¹´"
```

#### Split by tokens

è¿™ä¸ªå°±æ˜¯ä½¿ç”¨ NLP ä¸­ token è¿›è¡Œåˆ‡å‰², ä¸åŒçš„ tokenizer æœ‰ä¸åŒçš„åˆ‡å‰²æ–¹å¼. ä¸¾ä¸ªä¾‹å­, å¦‚æœä¸€ä¸ªå•è¯ç®—ä¸€ä¸ª token, é‚£å°±æŒ‰å•è¯åˆ‡å‰².
ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªå‘¢, å°±æ˜¯æœ‰äº› LLM çš„è¾“å…¥å…·æœ‰ token æ•°ç›®çš„é™åˆ¶, å› æ­¤æœ€å¥½åˆ†å‰²å­˜å‚¨çš„ tokenizer å’Œ LLM ä½¿ç”¨ä¸€æ ·çš„.

è¿™é‡Œä½¿ç”¨ OpenAI BPE tokenizer : tiktoken, æ˜¯[BPE ç®—æ³•](https://huggingface.co/learn/nlp-course/chapter6/5)çš„ä¸€ä¸ªå®ç°.

Split by tokens çš„ä½¿ç”¨æ–¹æ³•åŸºäºä¸Šè¾¹ 2 ç§ Splitter, åªæ˜¯åˆ‡å‰²æ—¶è°ƒç”¨æ–¹æ³•ä¸åŒ:

```python
# pip install tiktoken
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    encoding="cl100k_base", chunk_size=100, chunk_overlap=0
) # CharacterTextSplitter å®é™…ä¸å— chunk_size çš„çº¦æŸ
texts = text_splitter.split_text("text")

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name="gpt-4",
    chunk_size=100,
    chunk_overlap=0,
) # èƒ½å¤Ÿä¿è¯å­å¥å…¨éƒ¨å°äºchunk_size
texts = text_splitter.split_text("text")
# æ­¤å¤– encodingå‚æ•° å’Œ model_nameå‚æ•° æ•ˆæœç±»ä¼¼, å…·ä½“è¯·å‚è€ƒapi
```

#### Semantic Chunking

è¿™ä¸ªå°±æ˜¯å­—é¢æ„æ€, åŸºäº text ä¹‹é—´çš„è¯­ä¹‰è¿›è¡Œåˆ‡å‰², ä½¿å¾—è¯­ä¹‰ç›¸è¿‘çš„å°½é‡åœ¨ä¸€ä¸ª chunk, ä½†æ˜¯è¿™ä¸ªç›®å‰(2024 å¹´ 4 æœˆ 27 æ—¥)æ˜¯ä¸ªå®éªŒæ€§åŠŸèƒ½. å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/)

ç”±äºè¿™ä¸ªéœ€è¦è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦, æ‰€ä»¥éœ€è¦è¿›è¡Œ embedding.

ä¾‹å­:

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings
text_splitter = SemanticChunker(OpenAIEmbeddings())
texts = text_splitter.split_text("text")
```

è¿™ä¸ªé‡Œè¾¹æä¾›äº†ä¸€ä¸ª Breakpoints, ç”¨äºè¯„ä¼°ä»€ä¹ˆæ—¶å€™è¯¥åˆ‡å‰², è¯­ä¹‰ç›¸è¿‘å¤šå°‘æ‰ç®—ç›¸è¿‘?

- Percentile

Percentile(ç™¾åˆ†ä½æ•°) æ˜¯é»˜è®¤çš„è¯„ä¼°æ ‡å‡†, ä»–æ˜¯è®¡ç®—æ‰€æœ‰ä¸¤ä¸¤å¥å­ä¹‹é—´çš„ difference, å¦‚æœå¤§äºé˜ˆå€¼å°±ç»™ä»–åˆ‡å¼€.

ä¾‹å­:

```python
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), breakpoint_threshold_type="percentile"
    # breakpoint_threshold_amount : é»˜è®¤å€¼
)
```

é˜…è¯»[æºç ](https://api.python.langchain.com/en/latest/_modules/langchain_experimental/text_splitter.html#SemanticChunker)å¯ä»¥çœ‹åˆ°,å½“`threshold_type = "percentile"` æ—¶, é»˜è®¤ä½¿ç”¨ 95% åˆ†ä½æ•°. `breakpoint_threshold_amount` å‚æ•°æ§åˆ¶åˆ†ä½æ•°å…·ä½“å¤§å°.

- Standard Deviation

ç”¨æ³•ç±»ä¼¼, ä¸å†èµ˜è¿°. æºç ä¸­å½“`threshold_type = "standard_deviation"` æ—¶, é»˜è®¤ä½¿ç”¨ `mean + 3 * std` ä½œä¸ºé˜ˆå€¼. `breakpoint_threshold_amount` å‚æ•°æ§åˆ¶æ ‡å‡†å·®çš„å€æ•°.

- Interquartile

ä½¿ç”¨çš„ç®±çº¿å›¾æ–¹æ³•, é»˜è®¤ä½¿ç”¨ `mean + 1.5 * iqr`, å…¶ä¸­ `iqr = q3 - q1`, q3 ä¸º 75% åˆ†ä½æ•°, q1 ä¸º 25% åˆ†ä½æ•°. `breakpoint_threshold_amount` å‚æ•°æ§åˆ¶`q3 - q1`çš„å€æ•°.

### 2.3 Embedding


[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/text_embedding/)ç»™äº†å¾ˆå¤šç¬¬ä¸‰æ–¹ embedding  æ–¹æ³•. å…¶å®å°±æ˜¯è®­ç»ƒå¥½çš„ä¸€ä¸ª Matrix. è¿™é‡Œä½¿ç”¨ openAI æä¾›çš„ embedding.

ä¾‹å­:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings(api_key="...")
embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
embedded_query = embeddings_model.embed_query("What was the name mentioned in the conversation?")
embedded_query[:5]
```
è¾“å‡º:
```plaintext
[0.0053587136790156364,
 -0.0004999046213924885,
 0.038883671164512634,
 -0.003001077566295862,
 -0.00900818221271038]
```

#### Caching


åœ¨å¾—åˆ° embedding ä¹‹å, æˆ‘ä»¬å¯ä»¥å·²ç» embedding è¿‡çš„ token ç»™ä»–ç¼“å­˜, å¦‚æœåç»­åˆæ¥äº†åŒä¸€ä¸ª token, æˆ‘ä»¬å¯ä»¥ç›´æ¥ä» cache è°ƒç”¨, è€Œä¸å»éœ€è¦ä» embedding matrix è·å–.

æ ¸å¿ƒç»„ä»¶ä¸º CacheBackedEmbeddings, ä½¿ç”¨ä¾‹å­å¦‚ä¸‹:


```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import LocalFileStore
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter

underlying_embeddings = OpenAIEmbeddings()

store = LocalFileStore("./cache/") # è¡¨ç¤ºç¼“å­˜åˆ°æœ¬åœ°

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings, store, namespace=underlying_embeddings.model
)
"""
underlying_embedder: The embedder to use for embedding.
document_embedding_cache: Any ByteStore for caching document embeddings.
batch_size: (optional, defaults to None) The number of documents to embed between store updates.
namespace: (optional, defaults to "") The namespace to use for document cache. This namespace is used to avoid collisions with other caches. For example, you can set it to the name of the embedding model used.
"""
raw_documents = TextLoader("../../state_of_the_union.txt").load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
%%time
db = FAISS.from_documents(documents, cached_embedder)
# è¾“å‡º CPU times: user 218 ms, sys: 29.7 ms, total: 248 ms
# Wall time: 1.02 s

%%time
db2 = FAISS.from_documents(documents, cached_embedder)
# è¾“å‡º CPU times: user 15.7 ms, sys: 2.22 ms, total: 18 ms
# Wall time: 17.2 ms

```

æœ€å, store å¯ä»¥æ¢, æ¯”å¦‚ä½¿ç”¨ memory store:

```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import InMemoryByteStore
store = InMemoryByteStore()
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings, store, namespace=underlying_embeddings.model
)
```

### 2.4 Vector stores

[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/vectorstores/)æä¾›äº†è®¸å¤šç¬¬ä¸‰æ–¹çš„ Vector stores.

![image.png](https://s2.loli.net/2024/04/27/gBZulMS2hGEKJTi.png)

Facebook AI Similarity Search (FAISS) library, ä¾‹å­:

```python
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
query = "What did the president say about Ketanji Brown Jackson"
docs = db.similarity_search(query)
print(docs[0].page_content)
"""
ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ vector è¿›è¡Œ search
embedding_vector = OpenAIEmbeddings().embed_query(query)
docs = db.similarity_search_by_vector(embedding_vector)
print(docs[0].page_content) # è¾“å‡ºç»“æœæ˜¯ä¸€æ ·çš„
"""
```
è¾“å‡º:

```plaintext
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.

    Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.

    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```

#### Asynchronous operations

Vector Store ä¹Ÿæ”¯æŒ å¼‚æ­¥æ“ä½œ, `Qdrant` is a vector store, which supports all the async operations, thus it will be used in this walkthrough.

ä¾‹å­:

```python
# pip install qdrant-client
from langchain_community.vectorstores import Qdrant
db = await Qdrant.afrom_documents(documents, embeddings, "http://localhost:6333")
query = "What did the president say about Ketanji Brown Jackson"
docs = await db.asimilarity_search(query)
print(docs[0].page_content)
"""
# åŒç†æ”¯æŒ vector æŸ¥è¯¢
embedding_vector = embeddings.embed_query(query)
docs = await db.asimilarity_search_by_vector(embedding_vector)
"""

"""
# æ­¤å¤–è®¡ç®— similarity  çš„æ—¶å€™, æ”¯æŒ Maximum marginal relevance search (MMR)æ–¹æ³•:
query = "What did the president say about Ketanji Brown Jackson"
found_docs = await qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10)
for i, doc in enumerate(found_docs):
    print(f"{i + 1}.", doc.page_content, "\n")
"""
```
è¾“å‡º:

```plaintext
    Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.

    Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.

    One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

    And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```


### 2.5 Retrievers

Retrievers æ¥å—ç”¨æˆ·çš„ query, ç„¶åä» vector store ä¸­æ ¹æ®è§„åˆ™(ä¸åŒç§ç±»ç›¸ä¼¼åº¦)å»æœç´¢å¾—åˆ°åˆé€‚çš„ä¸Šä¸‹æ–‡, ç”¨äºåç»­å›ç­”è¾“å‡º.

åŒæ ·çš„, [å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/retrievers/)æœ‰å¤šç§ç±»å‹çš„ Retrievers, ä¸‹é¢ç®€è¦å­¦ä¹ .

#### Vector store-backed retriever

è¿™ä¸ª retriever æ˜¯æœ€ç®€å•çš„, ä»–ä½¿ç”¨çš„ search æ–¹æ³•æœ‰ similarity search and MMR.

> åç»­å…¶ä»–çš„é«˜çº§ retriever éƒ½æ˜¯åŸºäºè¿™ä¸ª retrieverè¿›è¡Œçš„åŒ…è£…. éƒ½æœ‰ä¸€ä¸ªå‚æ•° : base_retriever = retriever
{: .prompt-info }

ä¾‹å­:

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
loader = TextLoader("../../state_of_the_union.txt")
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(texts, embeddings)
retriever = db.as_retriever()
# retriever = db.as_retriever(search_type="mmr")
# retriever = db.as_retriever(
#     search_type="similarity_score_threshold", search_kwargs={"score_threshold": 0.5}
# )
# retriever = db.as_retriever(search_kwargs={"k": 1})

docs = retriever.invoke("what did he say about ketanji brown jackson")
```

#### MultiQueryRetriever

å‰è¾¹æåˆ°çš„æœ€ç®€å•çš„ retriever, å°†ç”¨æˆ·è¾“å…¥çš„ query å¯¹äº sotre ä¸­çš„æ–‡æœ¬è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—, ä½†æ˜¯æœ‰æ—¶å€™è¾“å…¥çš„ query å¯èƒ½å¹¶ä¸å¤ªæ˜ç¡®, å¯¼è‡´æœç´¢åˆ°çš„æ–‡æœ¬ä¸å¤Ÿæ¸…æ™°. è¿™æ—¶å¯ä»¥ä½¿ç”¨ MultiQueryRetriever, è¿™ä¸ª retriever å†…éƒ¨ä½¿ç”¨ä¸€ä¸ª LLM åŸºäºç”¨æˆ·è¾“å…¥çš„ query è¿›è¡Œåˆ†æ, è¾“å‡ºé€»è¾‘æ€§çš„è¿‡æ¸¡é—®é¢˜, è¿™æ ·æ¯ä¸ªé—®é¢˜éƒ½ä¼šåˆ†åˆ«å»ä¸ sotre ä¸­çš„èµ„æºè®¡ç®— similarity. é€šè¿‡å¯¹åŒä¸€é—®é¢˜ç”Ÿæˆå¤šä¸ªè§†è§’ï¼ŒMultiQueryRetriever æˆ–è®¸èƒ½å¤Ÿå…‹æœåŸºäºè·ç¦»çš„æ£€ç´¢çš„ä¸€äº›é™åˆ¶ï¼Œå¹¶è·å¾—æ›´ä¸°å¯Œçš„ç»“æœé›†ã€‚

ä¾‹å­:

```python
# Build a sample vectorDB
from langchain_chroma import Chroma
from langchain_community.document_loaders import WebBaseLoader
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_openai import ChatOpenAI

# Load blog post
loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
data = loader.load()
# Split
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
splits = text_splitter.split_documents(data)
# VectorDB
embedding = OpenAIEmbeddings()
vectordb = Chroma.from_documents(documents=splits, embedding=embedding)

question = "What are the approaches to Task Decomposition?"
llm = ChatOpenAI(temperature=0) # æŒ‡å®šä¸€ä¸ª LLM åŸºäº query ç”Ÿæˆå¤šè§’åº¦çš„ query
retriever_from_llm = MultiQueryRetriever.from_llm(
    retriever=vectordb.as_retriever(), llm=llm
)
unique_docs = retriever_from_llm.invoke(question)
len(unique_docs)
```
è¾“å‡º

```plaintext
# å¯ä»¥çœ‹åˆ°æ—¥å¿—é»˜è®¤ç”Ÿæˆäº†3ä¸ªé—®é¢˜(æ³¨æ„è¿™ä¸æ˜¯æœ€ç»ˆè¾“å‡ºå“ˆ~)
['1. How can Task Decomposition be approached?',
 '2. What are the different methods for Task Decomposition?',
 '3. What are the various approaches to decomposing tasks?']
```


#### Contextual Compression Retriever

è¿™ä¸ªå®é™…ä¸Šç®—æ˜¯ä¸€ä¸ª wrapper, æœ¬æ„æ˜¯ç”¨æ¥è§£å†³:å› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“ç”¨æˆ·åˆ°ä½æƒ³æœç´¢ä»€ä¹ˆ, æ‰€ä»¥ä»¥ä¼šç›´æ¥æ”¾å¤§é‡çš„æ–‡æ¡£ç»™ store ä¸­, ä½†æ˜¯è¿™å°±ä¼šå¯¼è‡´ä¸€ä¸ªé—®é¢˜, å½“æˆ‘ä»¬è¾“å…¥ query çš„æ—¶å€™, æœ‰ç”¨çš„ä¿¡æ¯å¯èƒ½ä¼šè¢«æ·¹æ²¡åœ¨å¤§é‡çš„æ–‡æ¡£ä¸­, è¿™å°±éœ€è¦æˆ‘ä»¬å¯¹æ–‡æ¡£ä¿¡æ¯è¿›è¡Œå‹ç¼©, æŠŠæ²¡ç”¨çš„ä¿¡æ¯è¿‡æ»¤æ‰.

ä¾‹å­:

```python
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import CharacterTextSplitter
def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"Document {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )
documents = TextLoader("../../state_of_the_union.txt").load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)
retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()

docs = retriever.invoke("What did the president say about Ketanji Brown Jackson")
pretty_print_docs(docs)

```
è¾“å‡º:
```plaintext
Document 1:

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
----------------------------------------------------------------------------------------------------
Document 2:

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder. Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.

We can do both. At our border, weâ€™ve installed new technology like cutting-edge scanners to better detect drug smuggling.

Weâ€™ve set up joint patrols with Mexico and Guatemala to catch more human traffickers.

Weâ€™re putting in place dedicated immigration judges so families fleeing persecution and violence can have their cases heard faster.

Weâ€™re securing commitments and supporting partners in South and Central America to host more refugees and secure their own borders.
----------------------------------------------------------------------------------------------------
Document 3:

And for our LGBTQ+ Americans, letâ€™s finally get the bipartisan Equality Act to my desk. The onslaught of state laws targeting transgender Americans and their families is wrong.

As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.

While it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year. From preventing government shutdowns to protecting Asian-Americans from still-too-common hate crimes to reforming military justice.

And soon, weâ€™ll strengthen the Violence Against Women Act that I first wrote three decades ago. It is important for us to show the nation that we can come together and do big things.

So tonight Iâ€™m offering a Unity Agenda for the Nation. Four big things we can do together.

First, beat the opioid epidemic.
----------------------------------------------------------------------------------------------------
Document 4:

Tonight, Iâ€™m announcing a crackdown on these companies overcharging American businesses and consumers.

And as Wall Street firms take over more nursing homes, quality in those homes has gone down and costs have gone up.

That ends on my watch.

Medicare is going to set higher standards for nursing homes and make sure your loved ones get the care they deserve and expect.

Weâ€™ll also cut costs and keep the economy going strong by giving workers a fair shot, provide more training and apprenticeships, hire them based on their skills not degrees.

Letâ€™s pass the Paycheck Fairness Act and paid leave.

Raise the minimum wage to $15 an hour and extend the Child Tax Credit, so no one has to raise a family in poverty.

Letâ€™s increase Pell Grants and increase our historic support of HBCUs, and invest in what Jillâ€”our First Lady who teaches full-timeâ€”calls Americaâ€™s best-kept secret: community colleges.
```
å¯ä»¥çœ‹åˆ°ç”±äºå­˜å‚¨çš„æ—¶å€™, chunk size æ¯”è¾ƒå¤§, å¹¶ä¸”æˆ‘ä»¬è¦æ‰¾çš„ä¿¡æ¯å°±ä»…ä»…ä¸ºä¸€å¥è¯(æ·¹æ²¡åœ¨æ–‡æ¡£ä¸­), æ‰€ä»¥ç®€å•çš„ä½¿ç”¨ retriever ä¼šç›´æ¥å°†ç›¸å…³çš„æ–‡æ¡£å…¨éƒ¨è¿”å›äº†. åœ¨ä¸Šè¾¹çš„åŸºç¡€ä¸Š, æˆ‘ä»¬å¯¹åŸºç¡€çš„ retriever è¿›è¡Œ warpper :

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain_openai import OpenAI

llm = OpenAI(temperature=0)
compressor = LLMChainExtractor.from_llm(llm)
# ç”¨äºå°†æŠ½åˆ°çš„ doc è¿›è¡Œ compression, å¹¶ä»æ¯ä¸ªæ–‡æ¡£ä¸­ä»…æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹ã€‚
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```
ä½†æ˜¯è¿™ä¸ª compression ä¼šä½¿ç”¨ LLM å¯¹æŠ½å›çš„æ–‡æ¡£è¿›è¡Œå¤„ç†å‹ç¼©(ä¸‡ä¸€ä»–å¤„ç†çš„ä¸å¥½å‘¢?). å®˜æ–¹æä¾›äº†ä¸€ç§å¯ä»¥ä¸æ”¹å˜åŸå§‹æ–‡æ¡£, ä½†èƒ½ä¿ç•™æ ¸å¿ƒä¿¡æ¯çš„ : filters.

ä¾‹å­:

```python
from langchain.retrievers.document_compressors import LLMChainFilter

_filter = LLMChainFilter.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=_filter, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```
è¾“å‡º:

```plaintext
Document 1:

Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while youâ€™re at it, pass the Disclose Act so Americans can know who is funding our elections.

Tonight, Iâ€™d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyerâ€”an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nationâ€™s top legal minds, who will continue Justice Breyerâ€™s legacy of excellence.
```

ä¸è¿‡è¿™ä¸ª filter åœ¨è¿‡æ»¤çš„æ—¶å€™, æ˜¯æŠŠæ•´ä¸ªæ–‡æœ¬å†åƒè¿›å»æ“ä½œ, å¯èƒ½å¸¦æ¥æ›´å¤šçš„ token è®¡ç®—é‡. EmbeddingsFilter å¯ä»¥ç›´æ¥ä½¿ç”¨ embedding è¿›è¡Œæ“ä½œ.

ä¾‹å­:

```python
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=embeddings_filter, base_retriever=retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```

æœ€å, å®˜æ–¹æä¾›ä¸€ä¸ª Pipeline èƒ½å¤ŸæŠŠ splitter, embedding, filter, retriever æ•´åˆ°ä¸€èµ·.

ä¾‹å­

```python
from langchain.retrievers.document_compressors import DocumentCompressorPipeline
from langchain_community.document_transformers import EmbeddingsRedundantFilter
from langchain_text_splitters import CharacterTextSplitter

splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=". ")
# EmbeddingsRedundantFilter å†…éƒ¨å®ç°å¯¹æ–‡æœ¬çš„ embedding å’Œ å»é‡å†—ä½™
redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)
# filter
relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)
# ç»„åˆ
pipeline_compressor = DocumentCompressorPipeline(
    transformers=[splitter, redundant_filter, relevant_filter]
)
# creat retriever
compression_retriever = ContextualCompressionRetriever(
    base_compressor=pipeline_compressor, base_retriever=retriever # å½“ç„¶è¦åŸºäºåŸºæœ¬çš„ retriever
)

compressed_docs = compression_retriever.invoke(
    "What did the president say about Ketanji Jackson Brown"
)
pretty_print_docs(compressed_docs)
```
è¾“å‡º:

```plaintext
Document 1:

One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.

And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson
----------------------------------------------------------------------------------------------------
Document 2:

As I said last year, especially to our younger transgender Americans, I will always have your back as your President, so you can be yourself and reach your God-given potential.

While it often appears that we never agree, that isnâ€™t true. I signed 80 bipartisan bills into law last year
----------------------------------------------------------------------------------------------------
Document 3:

A former top litigator in private practice. A former federal public defender. And from a family of public school educators and police officers. A consensus builder
----------------------------------------------------------------------------------------------------
Document 4:

Since sheâ€™s been nominated, sheâ€™s received a broad range of supportâ€”from the Fraternal Order of Police to former judges appointed by Democrats and Republicans.

And if we are to advance liberty and justice, we need to secure the Border and fix the immigration system.

We can do both
```

## Reference
