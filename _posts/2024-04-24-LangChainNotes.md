---
title: Lang Chain Notes
date: 2024-04-24 19:0000 +0800
categories: [ Deep Learning ,  LLM,  LangChain]
tags: [deep learning , llm ,  langchain]     # TAG names should always be lowercase
math: true
---


## 0. å‰è¨€


æœ¬ç¯‡Blogä¸æ˜¯æ•™ç¨‹,  å®˜æ–¹æ•™ç¨‹[1](https://python.langchain.com/docs/modules/)ã€[2](https://python.langchain.com/docs/get_started/introduction) æœ‰æ—¶å€™æ¯”è¾ƒæŠ½è±¡,  è¿™é‡Œåªæ˜¯åœ¨å­¦ä¹ LangChainè¿‡ç¨‹ä¸­åšä¸ªè®°å½•,  å¹¶åŠ å…¥è‡ªå·±çš„ç†è§£å’Œæ³¨é‡Š. å½“ç„¶è¿™é‡Œä¹Ÿä¸è¿›è¡Œè¿‡å¤šçš„ä»‹ç», ç›´æ¥è¿›å…¥å¯¹ç»„ä»¶çš„å­¦ä¹ .



## 1. Prompts

Prompts é€šå¸¸ç”¨æ¥"è°ƒæ•™"LLM,  æ¯”å¦‚ç”¨æ¥æŒ‡å®šLLMçš„è¾“å‡ºæ ¼å¼, ä¹Ÿå¯ä»¥ç”¨æ¥ç»™LMMä¸€äº›ä¾‹å­è®©ä»–å‚è€ƒç­‰ç­‰.LangChain ç›®å‰æä¾›äº† 4 ç§ Prompts template ,  æ–¹ä¾¿ç”¨æˆ·æ„é€ Prompt.

### 1.1 PromptTemplate

PromptTemplate æ˜¯æœ€ç®€å•,  æœ€åŸºæœ¬çš„ä¸€ç§ Template. API Reference:[PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)

å®˜æ–¹ç»™äº†2ç§æ–¹æ³•æ¥ç”¨PromptTemplate.

- æ–¹æ³•ä¸€(æ¨è)

```python
from langchain_core.prompts import PromptTemplate

# Instantiation using from_template (recommended)
prompt = PromptTemplate.from_template("Say {foo}")
prompt.format(foo="bar")
```

ç»“æœ : `Say bar`

- æ–¹æ³•äºŒ

```python
from langchain_core.prompts import PromptTemplate

# Instantiation using initializer
prompt = PromptTemplate(input_variables=["foo"],  template="Say {foo}")
prompt.format(foo="bar")

```

ç»“æœ : `Say bar`

### 1.2 ChatPromptTemplate

 ChatPromptTemplate é€šå¸¸æœ‰ 3 ç§è§„åˆ™: "system",  "ai" and "human". API Reference:[ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html)


```python

from langchain_core.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        ("system",  "You are a helpful AI bot. Your name is {name}."), 
        ("human",  "Hello,  how are you doing?"), 
        ("ai",  "I'm doing well,  thanks!"), 
        ("human",  "{user_input}"), 
    ]
)

messages = chat_template.format_messages(name="Bob",  user_input="What is your name?")
```
è¾“å‡º:

```python

[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), 
 HumanMessage(content='Hello,  how are you doing?'), 
 AIMessage(content="I'm doing well,  thanks!"), 
 HumanMessage(content='What is your name?')]
```

å¯ä»¥çœ‹åˆ°, å®é™…æ˜¯äº§ç”Ÿäº† `SystemMessage`,  `HumanMessage` and  `AIMessage` å…± 3 ç§ Message. ä¸ä¹‹å¯¹åº”çš„,  åœ¨æ„é€ è¿™äº›Messageæ—¶,  å¯ä»¥ä½¿ç”¨ç›¸åº”çš„ Template : `AIMessagePromptTemplate`, `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`

ä¾‹å­:

```python
from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate

chat_template = ChatPromptTemplate.from_messages(
    [
        SystemMessagePromptTemplate.from_template("ä½ æ˜¯ä¸€ä¸ª{llm_type}"),  # ä½¿ç”¨ templateæ„é€  ç³»ç»Ÿçš„ message
        ('ai', "å¾ˆé«˜å…´å¸®åŠ©æ‚¨."),  # ç›´æ¥ä½¿ç”¨ role æ„é€  AIçš„ message
        HumanMessagePromptTemplate.from_template("{text}"), 
    ]
)
messages = chat_template.format_messages(llm_type="AIåŠ©æ‰‹", text = "1 + 1 = ?")
print(messages)

```

è¾“å‡º:

```python
[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹'),  
AIMessage(content='å¾ˆé«˜å…´å¸®åŠ©æ‚¨.'), 
 HumanMessage(content='1 + 1 = ?')]
```


### 1.3 Example selectors

å‡è®¾æœ‰ä¸€äº›ä¾‹å­, æˆ‘ä»¬å¸Œæœ› LLM èƒ½å¤Ÿæ ¹æ®è¾“å…¥æŒ‘ä¸€äº›åˆé€‚çš„ä¾‹å­å‡ºæ¥, æ–¹ä¾¿æˆ‘ä»¬åç»­çš„æ“ä½œ, æ¯”å¦‚å°†ä»–ä»¬æ”¾åˆ°ä¸€ä¸ª prompt ä¸­. [API Reference](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)

####  Select by length

ç®€å•æ¥è¯´, è¿™ä¸ªselector æ˜¯æ ¹æ®ç”¨æˆ·è¾“å…¥çš„è¯­å¥é•¿çŸ­é€‰æ‹©åˆé€‚çš„example, ä½ è¾“å…¥çš„è¶ŠçŸ­,ä»–ç»™çš„ä¾‹å­è¶Šå¤š, ä½ è¾“å‡ºçš„è¶Šé•¿,ä»–ç»™çš„ä¾‹å­è¶Šå°‘.

ä¾‹å­:

```python
from langchain_core.example_selectors import LengthBasedExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

# ç”¨äºå°† example format
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)
example_selector = LengthBasedExampleSelector(
    examples=examples,
    example_prompt=example_prompt,
    max_length=25,
    # The function used to get the length of a string, which is used
    # to determine which examples to include. It is commented out because
    # it is provided as a default value if none is specified.
    # get_text_length: Callable[[str], int] = lambda x: len(re.split("\n| ", x))
)

```
è¾“å…¥:
```python
example_selector.select_examples({"input": "okay"})
```

è¾“å‡º:

```plaintext
[{'input': 'happy', 'output': 'sad'},
 {'input': 'tall', 'output': 'short'},
 {'input': 'energetic', 'output': 'lethargic'},
 {'input': 'sunny', 'output': 'gloomy'},
 {'input': 'windy', 'output': 'calm'}]

```

è¾“å…¥:
```python
long_string = "big and huge and massive and large and gigantic and tall and much much much much much bigger than everything else"
example_selector.select_examples({"input": long_string})
```

è¾“å‡º:

```plaintext
# å¯ä»¥çœ‹åˆ°åªç»™äº†ä¸€ä¸ªexample
[{'input': 'happy', 'output': 'sad'}]
```
#### Select by maximal marginal relevance (MMR)

è¿™ä¸ª selector çš„æ€æƒ³æ˜¯, é€‰æ‹©ä¸å½“å‰è¾“å…¥ p ç›¸ä¼¼çš„(cosine similarity) example $q_j$ , ä½†æ˜¯è¿™ä¸ª $q_j$ è¿˜è¦å°½é‡ä¸ example pool ä¸­ $q_i$ ä¸è¦å¤ªç›¸ä¼¼, è¿™æ˜¯ä¸ºäº†å¤šæ ·æ€§. [åŸå§‹paper](https://arxiv.org/pdf/2211.13892.pdf) ä¸­çš„ next example é€‰æ‹©å…¬å¼ä¸º:

![image.png](https://s2.loli.net/2024/04/25/2bHXmLKFRyAkxl5.png)

å¯ä»¥çœ‹åˆ°, å¦‚æœä¸‹ä¸€ä¸ª $q_j$ å’Œ å½“å‰çš„ $p$ å¾ˆç›¸ä¼¼, ä½†æ˜¯å’Œå…¶ä»–çš„ $q_i$ ä¹Ÿéå¸¸ç›¸ä¼¼, é‚£ä¹ˆè¿™ä¸ªåˆ†æ•°ä¹Ÿä¸ä¼šå¤ªé«˜. 

ä¾‹å­

```python

from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAIEmbeddings

# ç”¨äº format examples pool 
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_selector = MaxMarginalRelevanceExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    FAISS,
    # The number of examples to produce.
    k=2,
)

```
è¾“å‡º:

``` plaintext
[{'input': 'happy', 'output': 'sad'},
 {'input': 'windy', 'output': 'calm'}]
```

#### Select by similarity

è¿™ä¸ªå°±å¾ˆå•çº¯äº†, ç›´æ¥ä½¿ç”¨ cos similarity æ¥é€‰æ‹©æœ€ä½³çš„example, å…¸å‹ selector æ˜¯ SemanticSimilarityExampleSelector.

ä¾‹å­
```python

from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate
from langchain_openai import OpenAIEmbeddings

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # The number of examples to produce.
    k=1,
)

example_selector.select_examples({"adjective": "large"})

```
è¾“å‡º

```plaintext
[{'input': 'tall', 'output': 'short'}]
```
#### Select by n-gram overlap

è¿™ä¸ªæ˜¯è®¡ç®—è¾“å…¥çš„ query å’Œ example çš„ similarity(0-1ä¹‹é—´), ç„¶åæ ¹æ®ç»™å®šçš„é˜ˆå€¼, ç»™å‡ºæ»¡è¶³æ¡ä»¶çš„example.

é˜ˆå€¼ä¸º 0.0 è¡¨ç¤ºåªæ’é™¤ä¸ç›¸å…³çš„ example. é˜ˆå€¼ä¸º -1.0, è¡¨ç¤ºæ‰€æœ‰çš„ example éƒ½ä¼šè¿”å›. å¤§äº 1.0 è¡¨ç¤ºä¸è¿”å› example, é»˜è®¤ä¸º -1.0

ä¾‹å­

```python
from langchain_community.example_selector.ngram_overlap import (
    NGramOverlapExampleSelector,
)
from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate

example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)

# Examples of a fictional translation task.
examples = [
    {"input": "See Spot run.", "output": "Ver correr a Spot."},
    {"input": "My dog barks.", "output": "Mi perro ladra."},
    {"input": "Spot can run.", "output": "Spot puede correr."},
]
example_selector = NGramOverlapExampleSelector(
    # The examples it has available to choose from.
    examples=examples,
    # The PromptTemplate being used to format the examples.
    example_prompt=example_prompt,
    # The threshold, at which selector stops.
    # It is set to -1.0 by default.
    threshold=-1.0,
    # For negative threshold:
    # Selector sorts examples by ngram overlap score, and excludes none.
    # For threshold greater than 1.0:
    # Selector excludes all examples, and returns an empty list.
    # For threshold equal to 0.0:
    # Selector sorts examples by ngram overlap score,
    # and excludes those with no ngram overlap with input.
)

# è™½ç„¶  "My dog barks." ä¸ è¾“å…¥ä¸ç›¸å…³, ä½†æ˜¯è¿˜æ˜¯è¾“å‡ºäº† 
example_selector.select_examples({"sentence": "Spot can run fast."})

```

è¾“å‡º :

```plaintext
[{'input': 'Spot can run.', 'output': 'Spot puede correr.'},
 {'input': 'See Spot run.', 'output': 'Ver correr a Spot.'},
 {'input': 'My dog barks.', 'output': 'Mi perro ladra.'}]
```

è¾“å…¥:

```python
# ä¸è¾“å‡º ä¸ç›¸å…³çš„ 
example_selector.threshold = 0.0
print(dynamic_prompt.format(sentence="Spot can run fast."))

```
è¾“å‡º:
```plaintext
[{'input': 'Spot can run.', 'output': 'Spot puede correr.'},
 {'input': 'See Spot run.', 'output': 'Ver correr a Spot.'}]
```

### 1.3 Few-shot prompt templates

ä¸Šè¾¹ä»‹ç»äº†ä¸€äº› example selector , ç°åœ¨ä»‹ç» 
FewShotPromptTemplate + example selector .
äºŒè€…å®ç°çš„åŠŸèƒ½å°±æ˜¯, é¦–å…ˆæˆ‘ä»¬æœ‰ä¸€å † example (é€šå¸¸æ˜¯æˆå¯¹å„¿çš„è¾“å…¥å’Œè¾“å‡º) , ç„¶åå¯ä»¥è‡ªé€‚åº”çš„, æ ¹æ®ä¸åŒçš„è¾“å…¥, èƒ½å¤Ÿè‡ªåŠ¨çš„ select åˆé€‚çš„ example ä¸ è¾“å…¥åˆå¹¶, ä¸€åŒå˜ä¸º LLM çš„ prompt.

ä¹Ÿå°±æ˜¯è¯´, ä¸åŒçš„è¾“å…¥, ä¼šäº§ç”Ÿä¸åŒçš„ prompt , æˆ‘ç†è§£æ˜¯å’Œ Retrieval-augmented generation (RAG) ç±»ä¼¼çš„æ•ˆæœ. 

define example:

```python
from langchain_core.prompts.few_shot import FewShotPromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

examples = [
    {
        "question": "Who lived longer, Muhammad Ali or Alan Turing?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: How old was Muhammad Ali when he died?
Intermediate answer: Muhammad Ali was 74 years old when he died.
Follow up: How old was Alan Turing when he died?
Intermediate answer: Alan Turing was 41 years old when he died.
So the final answer is: Muhammad Ali
""",
    },
    {
        "question": "When was the founder of craigslist born?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the founder of craigslist?
Intermediate answer: Craigslist was founded by Craig Newmark.
Follow up: When was Craig Newmark born?
Intermediate answer: Craig Newmark was born on December 6, 1952.
So the final answer is: December 6, 1952
""",
    },
    {
        "question": "Who was the maternal grandfather of George Washington?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball
""",
    },
    {
        "question": "Are both the directors of Jaws and Casino Royale from the same country?",
        "answer": """
Are follow up questions needed here: Yes.
Follow up: Who is the director of Jaws?
Intermediate Answer: The director of Jaws is Steven Spielberg.
Follow up: Where is Steven Spielberg from?
Intermediate Answer: The United States.
Follow up: Who is the director of Casino Royale?
Intermediate Answer: The director of Casino Royale is Martin Campbell.
Follow up: Where is Martin Campbell from?
Intermediate Answer: New Zealand.
So the final answer is: No
""",
    },
]

# format example pool
example_prompt = PromptTemplate(
    input_variables=["question", "answer"], template="Question: {question}\n{answer}"
)

```

define selector:

```python
from langchain_chroma import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # This is the list of examples available to select from.
    examples,
    # This is the embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # This is the VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # This is the number of examples to produce.
    k=1,
)

```

FewShotPromptTemplate + example + selector : 

```python
prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    suffix="Question: {input}",
    input_variables=["input"],
)

print(prompt.format(input="Who was the father of Mary Ball Washington?"))
```

è¾“å‡º:

```plaintext
# å¯ä»¥çœ‹åˆ° prompt æœ€ååªå¼•å…¥äº†ä¸€ä¸ª example

Question: Who was the maternal grandfather of George Washington?

Are follow up questions needed here: Yes.
Follow up: Who was the mother of George Washington?
Intermediate answer: The mother of George Washington was Mary Ball Washington.
Follow up: Who was the father of Mary Ball Washington?
Intermediate answer: The father of Mary Ball Washington was Joseph Ball.
So the final answer is: Joseph Ball


Question: Who was the father of Mary Ball Washington?
```

### 1.5 Partial prompt templates

è¿™ä¸ªåŠŸèƒ½å°±æ˜¯ç±»ä¼¼å‡½æ•°çš„å‚æ•°å…·æœ‰é»˜è®¤å€¼. 

ä¾‹å­:

```python
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate.from_template("{foo} {bar}")
partial_prompt = prompt.partial(foo="666")
print(partial_prompt.format(bar="baz")) # è¾“å‡º 666 baz
```

è¿™ä¸ªä¹Ÿå¯ä»¥ä½¿ç”¨å‡½æ•°:

```python
from datetime import datetime

def _get_datetime():
    now = datetime.now()
    return now.strftime("%m/%d/%Y, %H:%M:%S")

prompt = PromptTemplate(
    template="Tell me a {adjective} joke about the day {date}",
    input_variables=["adjective", "date"],
)
# date é»˜è®¤ è°ƒç”¨å½“å‰æ—¶é—´
partial_prompt = prompt.partial(date=_get_datetime)
# è¾“å‡º : Tell me a funny joke about the day 04/25/2024, 23:22:18
print(partial_prompt.format(adjective="funny"))

```

### 1.6 PipelinePrompt

PipelinePrompt èƒ½å¤ŸæŠŠå¤šä¸ª prompt æ•´åˆ°ä¸€èµ·. 

```python
from langchain_core.prompts.pipeline import PipelinePromptTemplate
from langchain_core.prompts.prompt import PromptTemplate

# æœ€ç»ˆçš„ prompt
full_template = """{introduction}

{example}

{start}"""
full_prompt = PromptTemplate.from_template(full_template)

# ç”¨äº Introduction
introduction_template = """You are impersonating {person}."""
introduction_prompt = PromptTemplate.from_template(introduction_template)

# example 
example_template = """Here's an example of an interaction:

Q: {example_q}
A: {example_a}"""
example_prompt = PromptTemplate.from_template(example_template)

# ç”¨æˆ·çš„è¾“å…¥
start_template = """Now, do this for real!

Q: {input}
A:"""
start_prompt = PromptTemplate.from_template(start_template)

# å°†ä¸Šè¾¹çš„ prompt åˆå¹¶åˆ°ä¸€èµ·
input_prompts = [
    ("introduction", introduction_prompt),
    ("example", example_prompt),
    ("start", start_prompt),
]
# æŒ‡å®šè°æ˜¯è°
pipeline_prompt = PipelinePromptTemplate(
    final_prompt=full_prompt, pipeline_prompts=input_prompts
)
print(
    pipeline_prompt.format(
        person="Elon Musk",
        example_q="What's your favorite car?",
        example_a="Tesla",
        input="What's your favorite social media site?",
    )
)
```

è¾“å‡º :

```plaintext

You are impersonating Elon Musk.

Here's an example of an interaction:

Q: What's your favorite car?
A: Tesla

Now, do this for real!

Q: What's your favorite social media site?
A:

```


## 2. Retrieval

Retrieval Augmented Generation (RAG) å¯èƒ½æ˜¯ç›®å‰ LLM å‘æŒ¥æ¯”è¾ƒå¤§ä½œç”¨çš„ä¸€ä¸ªåº”ç”¨. å…¶æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å¤–æŒ‚çš„çŸ¥è¯†åº“èµ‹äºˆåœ¨ä¸åŒçš„å‚ç›´é¢†åŸŸåº”ç”¨èƒ½åŠ›. 

å…¶æ ¸å¿ƒæµç¨‹å¦‚ä¸‹:

[1] é¦–å…ˆæˆ‘ä»¬è¦æœ‰ç›¸åº”çš„èµ„æºåº“, source

[2] ç„¶åé’ˆå¯¹ä¸åŒçš„èµ„æº, æˆ‘ä»¬ä½¿ç”¨ç›¸åº”çš„ dataloader å°†èµ„æºæ•°æ®è¯»å–

[3] ç”±äºèµ„æºæ–‡æ¡£æ¯”è¾ƒé•¿, é€šå¸¸æˆ‘ä»¬è¦è¿›è¡Œåˆ†å—, ç§°ä¸º chunk

[4] å°†æ–‡æ¡£chunkå, ä¼šå¯¹æ¯ä¸ª chunk è¿›è¡Œ embedding

[5] embedding ä¹‹å, è¦è¿›è¡Œ store, è¿™ä¸ªç»„ä»¶ä¸€èˆ¬ç§°ä¸º vector store

[6] å½“æˆ‘ä»¬ç»™å®šè¾“å…¥çš„æ—¶å€™, LLM èƒ½å¤Ÿæ ¹æ®è¯­ä¹‰ä» store ä¸­æŠ½å–æœ‰ç”¨çš„èµ„æº,è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ retrieve

![image.png](https://s2.loli.net/2024/04/26/IiHL1MVWNc8QJtG.png)

å„ç§èµ„æºå°±ä¸ä»‹ç»äº†, æˆ‘ä»¬å­¦ä¹ ä¸åŒèµ„æºå¯¹åº”çš„ dataloader

### 2.1 Dataloader

[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/integrations/document_loaders/)é›†æˆäº†å¾ˆå¤šç¬¬ä¸‰æ–¹ dataloader, 
ç”šè‡³å¯ä»¥ç›´æ¥ä»arxivã€GitHubç­‰ç›´æ¥è·å–æ•°æ®. ä½†æ˜¯å¸¸ç”¨çš„å¯èƒ½å°±æ˜¯é’ˆå¯¹ æ–‡æœ¬ å’Œ csv çš„, è€Œä¸”ä½¿ç”¨æ–¹æ³•ç±»ä¼¼, æ‰€ä»¥è¿™é‡Œåªå­¦ä¹  æ–‡æœ¬ç±»å‹ çš„.

#### Document Loader

LangChain ç»™çš„ä¾‹å­æ˜¯ç»§æ‰¿ BaseLoader, ç„¶åå°†è¯»åˆ°çš„æ–‡æœ¬åˆå§‹åŒ–ä¸º Document å¯¹è±¡.
å†…éƒ¨æœ‰ 4 ä¸ªåŸºæœ¬æ–¹æ³•: ç›´æ¥è¯»å–æ‰€æœ‰, å¼‚æ­¥è¯»å–æ‰€æœ‰, lazy è¯»å–, å¼‚æ­¥lazyè¯»å–.

![image.png](https://s2.loli.net/2024/04/26/PC8fvdnsaHA9KB6.png){: width="400" height="300" }


ä¾‹å­:

<details markdown="1">
<summary> è¯¦ç»†ä¿¡æ¯ </summary>

```python
from typing import AsyncIterator, Iterator
from langchain_core.document_loaders import BaseLoader
from langchain_core.documents import Document

class CustomDocumentLoader(BaseLoader):
    """An example document loader that reads a file line by line."""

    def __init__(self, file_path: str) -> None:
        """Initialize the loader with a file path.

        Args:
            file_path: The path to the file to load.
        """
        self.file_path = file_path

    def lazy_load(self) -> Iterator[Document]:  # <-- Does not take any arguments
        """A lazy loader that reads a file line by line.

        When you're implementing lazy load methods, you should use a generator
        to yield documents one by one.
        """
        with open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

    # alazy_load is OPTIONAL.
    # If you leave out the implementation, a default implementation which delegates to lazy_load will be used!
    async def alazy_load(
        self,
    ) -> AsyncIterator[Document]:  # <-- Does not take any arguments
        """An async lazy loader that reads a file line by line."""
        # Requires aiofiles
        # Install with `pip install aiofiles`
        # https://github.com/Tinche/aiofiles
        import aiofiles

        async with aiofiles.open(self.file_path, encoding="utf-8") as f:
            line_number = 0
            async for line in f:
                yield Document(
                    page_content=line,
                    metadata={"line_number": line_number, "source": self.file_path},
                )
                line_number += 1

with open("./meow.txt", "w", encoding="utf-8") as f:
    quality_content = "meow meowğŸ± \n meow meowğŸ± \n meowğŸ˜»ğŸ˜»"
    f.write(quality_content)

loader = CustomDocumentLoader("./meow.txt")

## Test out the lazy load interface
for doc in loader.lazy_load():
    print()
    print(type(doc))
    print(doc)

## Test out the async implementation
async for doc in loader.alazy_load():
    print()
    print(type(doc))
    print(doc)

"""
è¾“å‡ºç»“æœä¸€æ ·:

<class 'langchain_core.documents.base.Document'>
page_content='meow meowğŸ± \n' metadata={'line_number': 0, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meow meowğŸ± \n' metadata={'line_number': 1, 'source': './meow.txt'}

<class 'langchain_core.documents.base.Document'>
page_content=' meowğŸ˜»ğŸ˜»' metadata={'line_number': 2, 'source': './meow.txt'}
"""
```
</details>

> load() can be helpful in an interactive environment such as a jupyter notebook.
Avoid using it for production code since eager loading assumes that all the content can fit into memory, which is not always the case, especially for enterprise data.
{: .prompt-info }

### 2.2 Text Splitters

Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window.

[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/document_transformers/)ç»™äº†å¤šç§ Splitter, æœ€å¸¸ç”¨çš„ä¸ºä»¥ä¸‹ 3 ç§.

#### Split by character

è¿™ä¸ªæ˜¯æœ€ç®€å•çš„ Splitter, å•çº¯å°±æ˜¯ä½¿ç”¨æŒ‡å®šçš„ character å»åˆ‡åˆ† text .

ä¾‹å­:

```python
from langchain_text_splitters import CharacterTextSplitter

text_splitter = CharacterTextSplitter(
    separator=" ", # æŒ‡å®š separator
    chunk_size=5,
    chunk_overlap=2,
    length_function=len,
    is_separator_regex=False,
)

text_splitter.split_text("æˆ‘æ˜¯ç»ƒä¹ æ—¶é•¿ è¾¾ä¸¤å¹´åŠçš„å¤å¤")

```
ä¸Šè¿°ä»£ç ä¸­, chunk_size æœ¬æ„æ˜¯æŒ‡è¿›è¡Œ split ä¹‹å, åç»­è¿›è¡Œstoreçš„æ—¶å€™, æœ€å¤§ä»¥å¤šå¤§çš„size ä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œå­˜å‚¨. ä½†æ˜¯å¯ä»¥çœ‹åˆ°è¿™ä¸ªå‚æ•°å¯¹äº CharacterTextSplitter ä¸ç”Ÿæ•ˆ, å®é™…ä¸Š[æºç ](https://api.python.langchain.com/en/latest/_modules/langchain_text_splitters/character.html#CharacterTextSplitter)ä¸­, å°±æ˜¯ç®€å•çš„ç”¨ re.split() å¯¹æ–‡æ¡£æŒ‰ç…§ separator è¿›è¡Œåˆ‡å‰², ä¸ç®¡å­å¥æœ‰å¤šé•¿, ç›´æ¥è¿”å›. 

è¾“å‡º:

```plaintext
['æˆ‘æ˜¯ç»ƒä¹ æ—¶é•¿', 'è¾¾ä¸¤å¹´åŠçš„å¤å¤']
```

> å› ä¸º LLM é€šå¸¸å¯¹è¾“å…¥æœ‰é•¿åº¦é™åˆ¶, å› æ­¤ CharacterTextSplitter ä¸å¤ªé€‚åˆ, å¯èƒ½ä¼šè¶…å‡ºè¾“å…¥å°ºå¯¸èŒƒå›´, è€Œä¸‹è¾¹çš„ RecursiveCharacterTextSplitter å¯ä»¥é€’å½’åˆ‡å‰²å­å¥, ç›´åˆ°æ¯ä¸ªå­å¥éƒ½å°äº chunk size.
{: .prompt-info }


#### Recursive Splitter By Character

è¿™ä¸ªçœ‹äº†[æºç ](https://api.python.langchain.com/en/latest/_modules/langchain_text_splitters/character.html#RecursiveCharacterTextSplitter),å®ƒé»˜è®¤çš„ `separator = ["\n\n", "\n", " ", ""]`, æˆ‘çš„ç†è§£æ˜¯è¯´, é¦–å…ˆä¼šæ ¹æ®`\n\n`è¿›è¡Œåˆ‡å‰². ä¸€èˆ¬æ¥è¯´, 2 ä¸ªæ¢è¡Œåˆ†å‰²å¼€çš„é€šå¸¸æ˜¯ 2 ç¯‡æ–‡ç« . æ‰€ä»¥ä¼šå…ˆæŒ‰ç…§è¿™ä¸ªå°ºåº¦è¿›è¡Œåˆ‡å‰². å¦‚æœåˆ‡å‰²ä¹‹å, æŸç¯‡æ–‡ç« è¿˜æ˜¯å¤ªé•¿(å¤§äº chunk size), é‚£ä¹ˆä¼šç»§ç»­ä½¿ç”¨ `\n` è¿›è¡Œåˆ’åˆ†åˆ‡å‰², åŒç†ç›´åˆ°å…¶é•¿åº¦å°äº chunk size.

ä¾‹å­:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size=5,
    chunk_overlap=0,
    length_function=len,
    is_separator_regex=False,
    keep_separator = False
)
text_splitter.split_text(
    "æˆ‘æ˜¯\n\nç»ƒä¹ æ—¶é•¿è¾¾ä¸¤å¹´\nåŠçš„å¤å¤"
    )
```
è¾“å‡º:
```plaintext
['æˆ‘æ˜¯', 'ç»ƒä¹ æ—¶é•¿è¾¾', 'ä¸¤å¹´', 'åŠçš„å¤å¤']
# å¯ä»¥çœ‹åˆ°, é¦–å…ˆç”¨`\n\n`è¿›è¡Œåˆ†å‰², å› ä¸º"æˆ‘æ˜¯"çš„é•¿åº¦å°äº5, æ‰€ä»¥ç›´æ¥å­˜èµ·æ¥, 
# ä½†æ˜¯åè¾¹éƒ¨åˆ†å¤ªé•¿, åˆåŸºäº`\n`è¿›è¡Œåˆ‡å‰², "åŠçš„å¤å¤"æ˜¯æ»¡è¶³è¦æ±‚çš„,æ‰€ä»¥ä¸€èµ·å­˜äº†èµ·æ¥.
# ä½†æ˜¯"ç»ƒä¹ æ—¶é•¿è¾¾ä¸¤å¹´"çš„é•¿åº¦è¿˜æ˜¯å¤§äº5, äºæ˜¯è¿›è¡Œäº†ç»§ç»­çš„åˆ‡å‰². å˜ä¸º"ç»ƒä¹ æ—¶é•¿è¾¾" å’Œ "ä¸¤å¹´"
```

#### Split by tokens

è¿™ä¸ªå°±æ˜¯ä½¿ç”¨ NLP ä¸­ token è¿›è¡Œåˆ‡å‰², ä¸åŒçš„ tokenizer æœ‰ä¸åŒçš„åˆ‡å‰²æ–¹å¼. ä¸¾ä¸ªä¾‹å­, åŠ å…¥ä¸€ä¸ªå•è¯ç®—ä¸€ä¸ªtoken, é‚£å°±æŒ‰å•è¯åˆ‡å‰².

è¿™é‡Œä½¿ç”¨ OpenAI BPE tokenizer : tiktoken, æ˜¯[BPE ç®—æ³•](https://huggingface.co/learn/nlp-course/chapter6/5)çš„ä¸€ä¸ªå®ç°.

Split by tokens çš„ä½¿ç”¨æ–¹æ³•åŸºäºä¸Šè¾¹ 2 ç§ Splitter, åªæ˜¯åˆ‡å‰²æ—¶è°ƒç”¨æ–¹æ³•ä¸åŒ:

```python
# pip install tiktoken
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    encoding="cl100k_base", chunk_size=100, chunk_overlap=0
) # CharacterTextSplitter å®é™…è¿˜æ˜¯å— chunk_size çš„çº¦æŸ
texts = text_splitter.split_text("text")

text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    model_name="gpt-4",
    chunk_size=100,
    chunk_overlap=0,
) # èƒ½å¤Ÿä¿è¯å­å¥å…¨éƒ¨å°äºchunk_size
texts = text_splitter.split_text("text")
# æ­¤å¤– encodingå‚æ•° å’Œ model_nameå‚æ•° æ•ˆæœç±»ä¼¼, å…·ä½“è¯·å‚è€ƒapi
```


#### Semantic Chunking

è¿™ä¸ªå°±æ˜¯å­—é¢æ„æ€, åŸºäº text ä¹‹é—´çš„è¯­ä¹‰è¿›è¡Œåˆ‡å‰², ä½¿å¾—è¯­ä¹‰ç›¸è¿‘çš„å°½é‡åœ¨ä¸€ä¸ªchunk, ä½†æ˜¯è¿™ä¸ªç›®å‰(2024å¹´4æœˆ27æ—¥)æ˜¯ä¸ªå®éªŒæ€§åŠŸèƒ½. å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/)

ç”±äºè¿™ä¸ªéœ€è¦è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦, æ‰€ä»¥éœ€è¦è¿›è¡Œ embedding. 

ä¾‹å­:

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai.embeddings import OpenAIEmbeddings
texts = text_splitter.split_text("text")
```

è¿™ä¸ªé‡Œè¾¹æä¾›äº†ä¸€ä¸ª Breakpoints, ç”¨äºè¯„ä¼°ä»€ä¹ˆæ—¶å€™è¯¥åˆ‡å‰², è¯­ä¹‰ç›¸è¿‘å¤šå°‘æ‰ç®—ç›¸è¿‘?

- Percentile

Percentile(ç™¾åˆ†ä½æ•°) æ˜¯é»˜è®¤çš„è¯„ä¼°æ ‡å‡†,  ä»–æ˜¯è®¡ç®—æ‰€æœ‰ä¸¤ä¸¤å¥å­ä¹‹é—´çš„difference, å¦‚æœå¤§äºé˜ˆå€¼å°±ç»™ä»–åˆ‡å¼€.

ä¾‹å­:
```python
text_splitter = SemanticChunker(
    OpenAIEmbeddings(), breakpoint_threshold_type="percentile"
    # breakpoint_threshold_amount : é»˜è®¤å€¼ 
)
```
é˜…è¯»[æºç ](https://api.python.langchain.com/en/latest/_modules/langchain_experimental/text_splitter.html#SemanticChunker)å¯ä»¥çœ‹åˆ°,å½“`threshold_type = "percentile"` æ—¶, é»˜è®¤ä½¿ç”¨ 95% åˆ†ä½æ•°. è¡¨ç¤ºå½“ 2 ä¸ªå¥å­å·®å¼‚æ€§å¤§äºæ‰€æœ‰è·ç¦»çš„ 95% åˆ†ä½æ•°æ—¶, å°±è¿›è¡Œåˆ‡å‰². `breakpoint_threshold_amount` å‚æ•°æ§åˆ¶åˆ†ä½æ•°å…·ä½“å¤§å°.

- Standard Deviation

ç”¨æ³•ç±»ä¼¼, ä¸å†èµ˜è¿°. æºç ä¸­å½“`threshold_type = "standard_deviation"` æ—¶, é»˜è®¤ä½¿ç”¨ `mean + 3 * std` ä½œä¸ºé˜ˆå€¼. è¡¨ç¤ºå½“ 2 ä¸ªå¥å­å·®å¼‚æ€§å¤§äºé˜ˆå€¼æ—¶, å°±è¿›è¡Œåˆ‡å‰².
`breakpoint_threshold_amount` å‚æ•°æ§åˆ¶æ ‡å‡†å·®çš„å€æ•°.
- Interquartile

ä½¿ç”¨çš„ç®±çº¿å›¾æ–¹æ³•, é»˜è®¤ä½¿ç”¨  `mean + 1.5 * iqr`, å…¶ä¸­ `iqr = q3 - q1`, q3 ä¸º 75% åˆ†ä½æ•°, q1 ä¸º 25% åˆ†ä½æ•°, 
`breakpoint_threshold_amount` å‚æ•°æ§åˆ¶`q3-q1`çš„å€æ•°.

## Reference








